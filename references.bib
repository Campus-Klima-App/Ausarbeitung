
@misc{google_scholar_google_nodate,
	title = {Google {Scholar} {Result} "{Cognitive} {Data} {Analysis} for {Big} {Data}"},
	url = {https://scholar.google.com/scholar?hl=de&as_sdt=0%2C5&q=Cognitive+Data+Analysis+for+Big+Data+jing+shyr&btnG=&oq=Cognitive+Data+Analysis+for+Big+Data+jin},
	urldate = {2020-07-21},
	author = {{Google Scholar}}
}

@incollection{shyr_cognitive_2018,
	title = {Cognitive {Data} {Analysis} for {Big} {Data}},
	booktitle = {Handbook of {Big} {Data} {Analytics}},
	publisher = {Springer},
	author = {Shyr, Jing and Chu, Jane and Woods, Mike},
	year = {2018},
	pages = {23--47}
}

@misc{shyr_jing_meet_nodate,
	title = {Meet {Jing} {Shyr}},
	url = {https://www.varicent.com/company/about-varicent/jing-shyr},
	language = {en},
	urldate = {2020-07-21},
	author = {{Shyr, Jing}},
	note = {Library Catalog: www.varicent.com}
}

@misc{noauthor_hadoop_2016,
	title = {Hadoop vs {Spark} vs {Flink} – {Big} {Data} {Frameworks} {Comparison}},
	url = {https://data-flair.training/blogs/hadoop-vs-spark-vs-flink/},
	abstract = {Hadoop vs Spark vs Flink tutorial-Difference between Spark vs Flink vs Hadoop, how Flink \& Spark are better than Hadoop \& what to choose Spark,Flink,Hadoop?},
	language = {en-US},
	urldate = {2020-07-21},
	journal = {DataFlair},
	month = dec,
	year = {2016},
	note = {Library Catalog: data-flair.training
Section: Apache Flink Tutorials}
}

@article{brooke_sus_1996,
	title = {{SUS} - {A} quick and dirty usability scale},
	volume = {Usability evaluation in industry},
	abstract = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for “quick and dirty” methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.},
	language = {en},
	author = {Brooke, John},
	year = {1996},
	pages = {189--194}
}

@incollection{hardle_cognitive_2018,
	address = {Cham},
	title = {Cognitive {Data} {Analysis} for {Big} {Data}},
	isbn = {978-3-319-18283-4 978-3-319-18284-1},
	url = {http://link.springer.com/10.1007/978-3-319-18284-1_2},
	abstract = {Cognitive data analysis (CDA) automates and adds cognitive processes to data analysis so that the business user or data analyst can gain insights from advanced analytics. CDA is especially important in the age of big data, where the data is so complex, and includes both structured and unstructured data, that it is impossible to manually examine all possible combinations. As a cognitive computing system, CDA does not simply take over the entire process. Instead, CDA interacts with the user and learns from the interactions. This chapter reviews IBM Corporation’s (IBM SPSS Modeler CRISP-DM guide, 2011) Cross Industry Standard Process for Data Mining (CRISP-DM) as a precursor of CDA. Then, continuing to develop the ideas set forth in Shyr and Spisic’s (“Automated data analysis for Big Data.” WIREs Comp Stats 6: 359–366, 2014), this chapter deﬁnes a new three-stage CDA process. Each stage (Data Preparation, Automated Modeling, and Application of Results) is discussed in detail. The Data Preparation stage alleviates or eliminates the data preparation burden from the user by including smart technologies such as natural language query and metadata discovery. This stage prepares the data for speciﬁc and appropriate analyses in the Automated Modeling stage, which performs descriptive as well as predictive analytics and presents the user with starting points and recommendations for exploration. Finally, the Application of Results stage considers the user’s purpose, which may be to directly gain insights for smarter decisions and better business outcomes or to deploy the predictive models in an operational system.},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {Handbook of {Big} {Data} {Analytics}},
	publisher = {Springer International Publishing},
	author = {Shyr, Jing and Chu, Jane and Woods, Mike},
	editor = {Härdle, Wolfgang Karl and Lu, Henry Horng-Shing and Shen, Xiaotong},
	year = {2018},
	doi = {10.1007/978-3-319-18284-1_2},
	note = {Series Title: Springer Handbooks of Computational Statistics},
	pages = {23--47}
}

@inproceedings{akil_usability_2017,
	address = {Boston, MA},
	title = {On the usability of {Hadoop} {MapReduce}, {Apache} {Spark} \& {Apache} flink for data science},
	isbn = {978-1-5386-2715-0},
	url = {http://ieeexplore.ieee.org/document/8257938/},
	doi = {10.1109/BigData.2017.8257938},
	abstract = {Distributed data processing platforms for cloud computing are important tools for large-scale data analytics. Apache Hadoop MapReduce has become the de facto standard in this space, though its programming interface is relatively low-level, requiring many implementation steps even for simple analysis tasks. This has led to the development of more advanced dataﬂow oriented platforms, most prominently Apache Spark and Apache Flink. Those platforms not only aim to improve performance through improved in-memory processing, but in particular provide built-in high-level data processing functionality, such as ﬁltering and join operators, which should make data analysis tasks easier to develop than with plain Hadoop MapReduce. But is this indeed the case? This paper compares three prominent distributed data processing platforms: Apache Hadoop MapReduce; Apache Spark; and Apache Flink, from a usability perspective. We report on the design, execution and results of a usability study with a cohort of master students, who were learning and working with all three platforms in order to solve different use cases set in a data science context. Our ﬁndings show that Spark and Flink are preferred platforms over MapReduce. Among participants, there was no signiﬁcant difference in perceived preference or development time between both Spark and Flink as platforms for batch-oriented big data analysis. This study starts an exploration of the factors that make Big Data platforms more – or less – effective for users in data science.},
	language = {en},
	urldate = {2020-07-05},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Akil, Bilal and Zhou, Ying and Rohm, Uwe},
	month = dec,
	year = {2017},
	pages = {303--310}
}

@book{brownlee_master_2016,
	title = {Master {Machine} {Learning} {Algorithms}},
	language = {en},
	author = {Brownlee, Jason},
	year = {2016}
}

@article{schwaber_scrum_2013,
	title = {Der {Scrum} {Guide}. {Der} gültige {Leitfaden} für {Scrum}: {Die} {Spielregeln}},
	url = {http://www.scrumguides.org/download.html},
	urldate = {2020-05-03},
	author = {Schwaber, Ken and Sutherland, Jeff},
	year = {2013}
}

@article{gebhart_challenges_2016,
	title = {Challenges of the digital transformation in software engineering},
	journal = {ICSEA 2016},
	author = {Gebhart, Michael and Giessler, Pascal and Abeck, Sebastian},
	year = {2016},
	pages = {149}
}

@incollection{sauter_agile_2018,
	title = {Agile {Arbeitswelt}},
	booktitle = {Agile {Werte}-und {Kompetenzentwicklung}},
	publisher = {Springer},
	author = {Sauter, Roman and Sauter, Werner and Wolfig, Roland},
	year = {2018},
	pages = {1--66}
}

@misc{beck_manifesto_2013,
	title = {Manifesto for {Agile} {Software} {Development}},
	url = {http://agilemanifesto.org/iso/en/manifesto.html},
	urldate = {2020-05-03},
	author = {Beck, Kent and Beedle, Mike and van Bennekum, Arie and Cockburn, Alistair and {Ward Cunningham} and {Martin Fowler} and {James Grenning} and {Jim Highsmith} and {Andrew Hunt} and {Ron Jeffries} and {Jon Kern} and {Brian Marick} and {Robert C. Martin} and {Steve Mellor} and {Ken Schwaber} and {Jeff Sutherland} and {Dave Thomas}},
	year = {2013}
}

@misc{alexander_von_humboldt_institut_fur_internet_und_gesellschaft_smart_2018,
	title = {Smart {Glasses} für besseren {Kundenservice} {\textbar} {Digitale} {Innovation} im {Mittelstand}},
	url = {https://www.youtube.com/watch?v=VZYXI70ku8M},
	urldate = {2020-04-28},
	author = {Alexander von Humboldt Institut für Internet und Gesellschaft},
	month = oct,
	year = {2018}
}

@article{hart_ray_1989,
	title = {Ray {Tracing} {Deterministic} 3-{D} {Fractals}},
	volume = {23},
	issn = {0097-8930},
	url = {https://doi.org/10.1145/74334.74363},
	doi = {10.1145/74334.74363},
	number = {3},
	journal = {SIGGRAPH Comput. Graph.},
	author = {Hart, J. C. and Sandin, D. J. and Kauffman, L. H.},
	month = jul,
	year = {1989},
	pages = {289--296}
}

@inproceedings{deppe_augmented_2018,
	address = {Düsseldorf, Germany},
	title = {Augmented reality for supporting manual non-destructive ultrasonic testing of metal pipes and plates},
	copyright = {Creative Commons Namensnennung - Weitergabe unter gleichen Bedingungen 4.0 Internationale Lizenz (CC-BY-SA)},
	url = {https://github.com/Ahrdie/ARinNDT/releases/tag/v1.1},
	abstract = {We describe an application of augmented reality technology for non-destructive testing of products in the metal-industry. The prototype is created with hard- and software, that is usually employed in the gaming industry, and delivers positions for creating ultra- sonic material scans (C-scans). Using a stereo camera in combination with an hmd enables realtime visualisation of the probes path, as well as the setting of virtual markers on the specimen. As a part of the implementation the downhill simplex optimization algorithm is implemented to fit the specimen to a cloud of recorded surface points. The accuracy is statistically tested and evaluated with the result, that the tracking system is accurate up to ca. 1-2 millimeters in well set-up conditions. This paper is of interest not only for research institutes of the metal-industry, but also for any areas of work, in which the enhancement with augmented reality is possible and a precise tracking is necessary.},
	author = {Deppe, Robert and Nemitz, Oliver and Herder, Jens},
	month = aug,
	year = {2018},
	keywords = {AR, Augmented Reality, NDT, Nondestructive Testing, Stereocamera, Tracking, Ultrasonic, Ultrasonic imaging}
}

@phdthesis{deppe_einsatz_2018,
	address = {Düsseldorf, Germany},
	type = {Bachelor {Thesis} ({Bachelorarbeit})},
	title = {Einsatz der {Augmented} {Reality} in der industriellen {Zerstörungsfreien} {Prüfung}},
	abstract = {The aim of this thesis is to describe the application of augmented reality technology in non–destructive testing of products of the metal–industry and to create a prototype. This prototype is created with hard– and software, that is usually employed in the gaming industry, and delivers positions for creating c– scans. Using the ZEDmini in combination with the HTC VIVE enables realtime visualisation of the probes path in the HMD, as well as the setting of virtual markers on the specimen. As a part of the implementation the downhill–simplex optimization–algorithm is implemented to fit the specimen to a cloud of recorded surfacepoints. The accuracy is statistically tested and evaluated with the result, that the VIVE–trackingsystem is accurate up to ca. 1–2 millimeters in well lit conditions. This thesis is of interest not only for research–institutes of the metal–industry, but also for any areas of work, in which the enhancement with augmented–reality is possible.},
	language = {german},
	school = {Hochschule Düsseldorf, University of Applied Sciences},
	author = {Deppe, Robert},
	year = {2018},
	keywords = {AR, Augmented Reality, HTC VIVE, NDT, Nondestructive Testing, Stereocamera, Tracking, Ultrasonic, ZEDmini, ZfP}
}

@misc{marczak_mandelbulber_2016,
	title = {Mandelbulber - {Tutorial} \& {User} {Manual}},
	url = {http://cdn.mandelbulber.org/doc/Mandelbulber_Manual_v091.pdf},
	urldate = {2020-01-30},
	author = {Marczak, Krzysztof and McLarekin, Graeme and Jennen, Sebastian},
	month = sep,
	year = {2016}
}

@phdthesis{kose_interaktive_2015,
	address = {Düsseldorf},
	title = {Interaktive echtzeitfähige {3D}-{Fraktale}},
	language = {Deutsch},
	school = {Fachhochschule Düsseldorf},
	author = {Köse, Okan Sadik},
	year = {2015}
}

@inproceedings{watters_raymarching_2018,
	address = {Vancouver, British Columbia, Canada},
	title = {Raymarching toolkit for unity: a highly interactive unity toolkit for constructing signed distance fields visually},
	isbn = {978-1-4503-5819-4},
	shorttitle = {Raymarching toolkit for unity},
	url = {http://dl.acm.org/citation.cfm?doid=3214822.3214828},
	doi = {10.1145/3214822.3214828},
	abstract = {Raymarching signed distance fields is a technique used by graphics experts and demoscene enthusiasts to construct scenes with features unusual in traditional polygonal workflows–blending shapes, kaleidoscopic patterns, reflections, and infinite fractal detail all become possible and are represented in compact representations that live mostly on the graphics card. Until now these scenes have had to be constructed in shaders by hand, but the Raymarching Toolkit for Unity is an extension that combines Unity’s highly visual scene editor with the power of raymarched visuals by automatically generating the raymarching shader for the scene an artist is creating, live.},
	language = {en},
	urldate = {2019-10-23},
	booktitle = {{ACM} {SIGGRAPH} 2018 {Studio} on   - {SIGGRAPH} '18},
	publisher = {ACM Press},
	author = {Watters, Kevin and Ramallo, Fernando},
	year = {2018},
	pages = {1--2}
}

@inproceedings{hutchins_big_2015,
	address = {Los Angeles, California},
	title = {Big {Hero} 6: into the portal},
	isbn = {978-1-4503-3636-9},
	shorttitle = {Big {Hero} 6},
	url = {http://dl.acm.org/citation.cfm?doid=2775280.2792521},
	doi = {10.1145/2775280.2792521},
	language = {en},
	urldate = {2019-10-23},
	booktitle = {{ACM} {SIGGRAPH} 2015 {Talks} on - {SIGGRAPH} '15},
	publisher = {ACM Press},
	author = {Hutchins, David and Riley, Olun and Erickson, Jesse and Stomakhin, Alexey and Habel, Ralf and Kaschalk, Michael},
	year = {2015},
	pages = {1--1}
}

@inproceedings{ebb_building_2017,
	address = {Los Angeles, California},
	title = {Building detailed fractal sets for "{Guardians} of the {Galaxy} {Vol}. 2"},
	isbn = {978-1-4503-5008-2},
	url = {http://dl.acm.org/citation.cfm?doid=3084363.3085060},
	doi = {10.1145/3084363.3085060},
	abstract = {Building the digital sets for Guardians of the Galaxy Vol. 2 presented a unique challenge for Animal Logic’s asset and FX teams. e creative brief involved two separate alien environments made from complex mathematical shapes, with an unprecedented amount of detail. As an additional challenge the environments had to match the look and feel of speci cally styled concept art with a grand, monumental design. To meet the artistic requirements required a high level of creative control and manipulation of set elements that were to be rendered alongside many other highly detailed objects, and propagated quickly through the pipeline for fast feedback iterations. e team used a novel approach to modelling fractal objects using point clouds, taking advantage of pipeline capabilities to integrate FX objects with environment set-pieces. Additionally the team, leveraged instancing and high levels of geometric complexity using the in-house renderer Glimpse.},
	language = {en},
	urldate = {2019-10-20},
	booktitle = {{ACM} {SIGGRAPH} 2017 {Talks} on   - {SIGGRAPH} '17},
	publisher = {ACM Press},
	author = {Ebb, Matt and Sutherland, Richard and Heckenberg, Daniel and Green, Miles},
	year = {2017},
	pages = {1--2}
}

@inproceedings{lomas_cellular_2014,
	address = {Vancouver, Canada},
	title = {Cellular forms: an artistic exploration of morphogenesis},
	isbn = {978-1-4503-2977-4},
	shorttitle = {Cellular forms},
	url = {http://dl.acm.org/citation.cfm?doid=2619195.2656282},
	doi = {10.1145/2619195.2656282},
	language = {en},
	urldate = {2019-10-20},
	booktitle = {{ACM} {SIGGRAPH} 2014 {Studio} on - {SIGGRAPH} '14},
	publisher = {ACM Press},
	author = {Lomas, Andy},
	year = {2014},
	pages = {1--1}
}

@inproceedings{saam_fractal_2018,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '18},
	title = {Fractal {Multiverses} in {VR}},
	isbn = {978-1-4503-5820-0},
	url = {http://doi.acm.org/10.1145/3214745.3214813},
	doi = {10.1145/3214745.3214813},
	booktitle = {{ACM} {SIGGRAPH} 2018 {Talks}},
	publisher = {ACM},
	author = {Saam, Johannes and Merchante, Mariano and Beavers, Patrick and Jin, Rongxuan and Hjartarson, Aron and Bees, David},
	year = {2018},
	note = {event-place: Vancouver, British Columbia, Canada},
	keywords = {fractals, ray marching, reprojection, signed distance functions, virtual reality},
	pages = {64:1--64:2}
}

@inproceedings{mcgraw_fractal_2018,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '18},
	title = {Fractal {Anatomy}: {Imaging} {Internal} and {Ambient} {Structures}},
	isbn = {978-1-4503-5817-0},
	url = {http://doi.acm.org/10.1145/3230744.3230748},
	doi = {10.1145/3230744.3230748},
	booktitle = {{ACM} {SIGGRAPH} 2018 {Posters}},
	publisher = {ACM},
	author = {McGraw, Tim},
	year = {2018},
	note = {event-place: Vancouver, British Columbia, Canada},
	keywords = {computational art, fractal, generative art},
	pages = {9:1--9:2}
}

@misc{mathworks_find_nodate,
	title = {Find minimum of unconstrained multivariable function using derivative-free method - {MATLAB} fminsearch},
	url = {https://www.mathworks.com/help/matlab/ref/fminsearch.html},
	urldate = {2018-04-18},
	author = {MathWorks}
}

@misc{noauthor_valves_nodate,
	title = {Valve's "{Lighthouse}" {Tracking} {System} {May} {Be} {Big} {News} for {Robotics} {\textbar} {Hizook}},
	url = {http://www.hizook.com/blog/2015/05/17/valves-lighthouse-tracking-system-may-be-big-news-robotics},
	urldate = {2018-03-23}
}

@inproceedings{noauthor_9_2017,
	title = {(9) {I} {Am} {The} {Passenger}: {How} {Visual} {Motion} {Cues} {Can} {Influence} {Sickness} {For} {In}-{Car} {VR}},
	shorttitle = {(9) {I} {Am} {The} {Passenger}},
	url = {https://www.researchgate.net/publication/316708813_I_Am_The_Passenger_How_Visual_Motion_Cues_Can_Influence_Sickness_For_In-Car_VR},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2018-02-21},
	year = {2017}
}

@article{noauthor_role_2018,
	title = {The {Role} of {Haptics} in {User} {Input} for {Simple} {3D} {Interaction} {Tasks} - {An} {Analysis} of {Interaction} {Performance} and {User} {Experience}},
	doi = {10.5220/0006580500260037},
	year = {2018}
}

@misc{ebner_augmented_2016,
	title = {Die {Augmented} {Reality} {Ultraschall}-{App} {Studie} ({UppS})},
	url = {https://www.researchgate.net/publication/320052966_Die_Augmented_Reality_Ultraschall-App_Studie_UppS},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2018-03-21},
	journal = {ResearchGate},
	author = {Ebner, Florian},
	month = oct,
	year = {2016}
}

@inproceedings{skala-szymanska_use_2014,
	title = {Use of {Nelder}-{Mead} simplex method to arc fitting for railway track realignment},
	isbn = {978-609-457-640-9},
	doi = {10.3846/enviro.2014.244},
	abstract = {The development of technology implies the elaboration of new calculation methods. Such methods should be precise and efficient. The article presents a new method of railway track realignment. The example considered in the article consistsof an circular arc that is tangent to two straight lines. Authors show subsequent stages of realization of tasks of railway track realignment. Each formula is described in this paper. The novelty of this solution is use of the Nelder-Mead simplex method for fitting section of railway track geometry into set of measure points. The functional model of optimization problem was derived and discussed in details. The method of optimization problem was presented in the form of detailed algorithm.The example of use of proposed algorithm was presented and analysed.},
	author = {Skała-Szymańska, Marta and Cellmer, Slawomir and Rapinski, Jacek},
	month = jan,
	year = {2014}
}

@inproceedings{walter_non-contact_2007,
	address = {Québec City, Québeck},
	title = {Non-contact tracking of phased-array probe and real-time generation of {C}-scans for the inspection of {Composite} aerospace structures},
	url = {http://www.ndt.net/article/ndt-canada2017/papers/3._Walter_2017152-NDTinCanada_JulienWalter.pdf},
	abstract = {The proportion of composite materials used in modern aircraft primary structures is continuously increasing because of their high mechanical performance and low density allowing for significant weight savings. Various factors can harm composite parts during the in-service life of the aircraft (impacts, collisions, lightning strikes, dropped tools, etc.) causing subsurface damages such as delaminations or disbonds. The structural integrity should therefore be verified throughout the lifetime of the aircraft. Ultrasonic testing (UT) is the preferred method to perform non-destructive inspection of composite aerospace components. Most of the in-service inspections are performed using manual UT techniques, which are easily deployed and often give satisfying results but also have strong limitations, the main one being the impossibility to get images as inspection results. Obtaining C-scan is highly desirable for in-service applications in order to increase the probability of detection, improve the results traceability or provide guidance for repairs. Different phased-array solutions exist to obtain C-scans for in-service inspections but they require the use of mechanical encoders or scanners which also bring their own limitations: limited inspection surface, restrictions on the operator movement, problems adapting the scanner to curves and complex geometries inherent to an aircraft. We present a novel inspection technique using a portable phased-array UT instrument coupled to a dynamic tracking system based on stereoscopic cameras. The 3D vision device follows the trajectory of a custom target mounted on the phased-array probe. A software application was developed to perform real-time transformation of the 3D linear and angular coordinates into the 2D scan-index coordinates and skew angle required by the UT instrument. A complex double curvature aerospace component was inspected with the developed method. The UT probe was moved freely in any direction on the inspected surface (Paintbrush movement) without any physical encoder nor scanner attached to it. Ultrasonic C-scans were generated. The accuracy of the defects positioning and sizing as well as the overall performance of the method are discussed.},
	publisher = {Centre Technologique en Aérospatiale},
	author = {Walter, J and Beausoleil, A and Arès, O and Brouillette, T and Poulin-Gagnon, D and Morrow, F},
	month = jun,
	year = {2007},
	pages = {28}
}

@book{moles_introduction_2004,
	address = {Waltham, MA, USA},
	edition = {3},
	title = {Introduction to {Phased} {Array} {Ultrasonic} {Technology} {Applications}},
	language = {english},
	publisher = {Olympus NDT},
	author = {Moles, Michael D.C. and Bird, Colin R. and Herzog, Pamela and Armitt, Tim and Ciorau, Petru and Roberts, Ron and Davis, Mark},
	year = {2004}
}

@inproceedings{amza_augmented_2017,
	address = {Romania},
	title = {Augmented reality application for training in pipe defects ultrasonic investigation},
	volume = {121},
	copyright = {CC-BY},
	doi = {10.1051/matecconf/201712104001},
	abstract = {The paper presents the development process of an Augmented Reality (AR) application used for training operators in using ultrasonic equipment for non-destructive testing (NDT) of pipework. The application provides workers useful information regarding the process steps, the main components of ultrasonic equipment and the proper modality of placing, aligning and moving it on pipe and weld. Using tablet or mobile phone device, an operator can see on screen written details and images on standardized working method, thus offering assistance during the training process. Allowing 3D augmented visualization of ultrasonic equipment overlaid on the real-world environment consisting in pipes and welds, the AR application makes the NDT process easier to understand and learn, as the initial evaluation results showed.},
	author = {Amza, Catalin Gheorghe and Zapciu, Aurelian and Popescu, Diana and Teodorescu, Octav},
	month = jan,
	year = {2017}
}

@patent{langlois_system_2015,
	title = {A system and method of non-destructive inspection with a visual scanning guide},
	abstract = {of guiding an inspection probe according to a predeter- mined inspection plan. The device is couple with a probe which is to be moved according to the inspection plan on the test object, the device including an inspection guide unit having a guide control unit, a position encoding such as a 3-D camera and visual feedback eyewear. The meth- od including facilitating a virtual display of the inspection plan onto the visual feedback eyewear, moving the probe following the virtual display of the inspection plan, sens- ing sensed probe positions in real time of the inspection using the 3-D camera and validating the sensed probe position against the inspection plan using the control module. Then the information of the step of validating, such as those spots at which the probe is moved out of the tolerance of the inspection plan, is displayed on the feedback eyewear.},
	language = {English},
	number = {EP 2 846 158 A2},
	author = {Langlois, Pierre and Lepage, Benoît and St-Laurent, Martin and Habermehl, Jason},
	month = nov,
	year = {2015},
	pages = {12}
}

@article{aurand_accuracy_2017,
	title = {Accuracy map of an optical motion capture system with 42 or 21 cameras in a large measurement volume},
	volume = {58},
	abstract = {Erforschung der Genauigkeit von Optitrack},
	language = {english},
	journal = {Journal of Biomechanics},
	author = {Aurand, Alexander M. and Dufour, Jonathan S. and Marras, William S.},
	month = aug,
	year = {2017},
	pages = {4}
}

@inproceedings{fadzil_design_2015,
	title = {Design, simulation, and kinematic analysis of a manipulator-based {3D} position tracking system},
	doi = {10.1109/IRIS.2015.7451586},
	abstract = {In recent decades, the use of three-dimensional ultrasound (3DUS) imaging of organs has been growing rapidly compared to its two-dimensional ultrasound (2DUS) type. The use of freehand technique in 3DUS system may help to improve the natural flexibility performance. To realize a 3DUS freehand imaging, a position tracking system which measures the US probe position in real time is added. The design, simulation, and analysis of a new 6 degree-of-freedom (6DOF) manipulator-based position tracking system for 3DUS probe are presented in this paper. The forward kinematics of the manipulator was calculated using the Denavit-Hartenberg (D-H) convention. The position of end-effector, which represented the US probe position, was then measured using the joint parameters. The mathematical equations were then verified using Robotic Toolbox in MATLAB as the simulation software. The results of both simulation and equation verified the validity of the kinematic equations.},
	booktitle = {2015 {IEEE} {International} {Symposium} on {Robotics} and {Intelligent} {Sensors} ({IRIS})},
	author = {Fadzil, Muhaimin Mohd and Faudzi, 'Athif Mohd and Sayahkarajy, Mostafa and Shamsudin, Mohamad Amir and Dewi, Dyah Ekashanti Octorina and Supriyanto, Eko},
	month = oct,
	year = {2015},
	keywords = {3D position tracking system, 3DUS freehand imaging, 3DUS imaging, 3DUS probe, 6 degree-of-freedom manipulator, 6DOF manipulator, Biomedical imaging, D-H convention, Denavit-Hartenberg convention, Interference, MATLAB, Mathematical model, Probes, Robotic Toolbox, US probe position measurement, Ultrasonic imaging, Ultrasonic variables measurement, biological organs, biomedical ultrasonics, design, end effectors, end-effector, forward kinematics, freehand technique, kinematic analysis, kinematic equations, manipulator kinematics, manipulator-based position tracking, mathematical equations, medical image processing, medical robotics, natural flexibility performance improvement, organs, simulation software, three-dimensional freehand ultrasound imaging, three-dimensional ultrasound imaging},
	pages = {55--59}
}

@misc{okreylos_hacking_2014,
	title = {Hacking the {Oculus} {Rift} {DK2}, part {III}},
	url = {http://doc-ok.org/?p=1138},
	abstract = {Note: This is part 3 of a four-part series. [Part 1] [Part 2] [Part 4] In the previous part of this ongoing series of posts, I described how the Oculus Rift DK2’s tracking LEDs can be identif…},
	urldate = {2017-12-18},
	journal = {Doc-Ok.org},
	author = {{okreylos}},
	month = oct,
	year = {2014}
}

@misc{nemitz_bildgebende_2013,
	title = {Bildgebende {Ultraschallprüfung} zur verbesserten {Fehlercharakterisierung} bei der {Schweißnahtprüfung} von längsnahtgeschweißten {Großrohren}},
	language = {german},
	author = {Nemitz, Oliver and Graff, Alfred and Schmitte, Till and Chichkov, Nikolai and Orth, Thomas and Kersting, Thomas and Dillhöfer, Alexander and Rieder, Hans and Spies, Martin},
	month = nov,
	year = {2013}
}

@inproceedings{islam_indoor_2016,
	title = {Indoor positional tracking using dual-axis rotating laser sweeps},
	doi = {10.1109/I2MTC.2016.7520559},
	abstract = {Accurate positioning of objects within an indoor environment is essential for applications such as virtual reality, robotics and multirotor drones. Previous methods of obtaining six-degrees-of-freedom for a tracked object have been susceptible to latency and accumulated error, computationally intensive, inaccurate, costly, or required exotic components, setups and calibration procedures. This paper presents a novel architecture and implementation of an indoor tracking system that consists of a rotary-laser base station and a tracked object equipped with photodiode sensors. The base station emits horizontal and vertical laser lines that sweep across the environment in sequence. The base station also emits an infrared synchronization beacon that floods the environment between each sweep. The tracked object consists of multiple photodiode sensors and a processing unit. Based on the timings between the synchronization beacons and the sweeps observed by each photodiode, along with the known configuration of the photodiode constellation, the position and orientation of the tracked object can be determined with high accuracy, low latency, and low computational overhead. In addition, the system allows a large number of such objects to be tracked within the same space, as each tracked object can be a separate embedded device. The Nikon iGPS tracking system, along with the more recently announced Lighthouse technology from Valve Corporation, use multiple base stations to triangulate a tracked object, while we will show that one is sufficient for basic tracking. In addition, this paper is the first to describe the specifications for a low-cost version of such a system. Observations and performance characteristics of the constructed prototypes are discussed.},
	booktitle = {2016 {IEEE} {International} {Instrumentation} and {Measurement} {Technology} {Conference} {Proceedings}},
	author = {Islam, Shahidul and Ionescu, Bogdan and Gadea, Christian and Ionescu, Dan},
	month = may,
	year = {2016},
	keywords = {Azimuth, Base stations, Global Positioning System, Lasers, Measurement by laser beam, Nikon iGPS tracking system, Photodiodes, Rotors, Synchronization, angle measurement, dual-axis rotating laser sweeps, horizontal laser lines, indoor navigation, indoor positional tracking, indoor positioning, infrared sensor, infrared synchronization, laser beacon, localization, measurement by laser beam, orientation, photodiode constellation, photodiode sensors, photodiodes, positioning, rotary-laser base station, synchronisation, synchronization beacons, tracked object, triangulation, vertical laser lines},
	pages = {1--6}
}

@inproceedings{sauer_augmented_2001,
	title = {Augmented reality visualization of ultrasound images: system description, calibration, and features},
	shorttitle = {Augmented reality visualization of ultrasound images},
	doi = {10.1109/ISAR.2001.970513},
	abstract = {We developed a system for augmented reality visualization of ultrasound images. This system is based on our earlier "augmented workspace" setup. The user wears a custom video-see-through head-mounted display (HMD). Two color video cameras attached to the HMD provide a stereo view of the scene. A third head-mounted video camera is added for tracking. A set of markers is attached to the ultrasound transducer; a set of stationary markers can be positioned above the workspace. The system runs at the full 30 Hz video frame rate with a latency of about 0.1 sec, generating a stable augmentation with no apparent jitter visible in the composite images. Three SGI Visual Workstations provide the computing power for the system. The authors describe the details of the system, its calibration with a configuration of optical markers partially immersed in a water bath, and some system features like spatiotemporal freezing and 3D target localization that promise to be helpful for practical applications},
	booktitle = {Proceedings {IEEE} and {ACM} {International} {Symposium} on {Augmented} {Reality}},
	author = {Sauer, Frank and Khamene, Ali and Bascle, Benedicte and Schinunang, Lars and Wenzel, Fabian and Vogt, Sebastian},
	year = {2001},
	keywords = {0.1 s, 30 Hz, 3D target localization, Augmented reality, Cameras, Delay, Displays, HMD, Jitter, Layout, SGI Visual Workstations, Ultrasonic imaging, Ultrasonic transducers, Visualization, Workstations, augmented reality, augmented reality visualization, augmented workspace, biomedical ultrasonics, color video cameras, composite images, computing power, custom video-see-through head mounted display, data visualisation, head-mounted video camera, helmet mounted displays, medical computing, spatiotemporal freezing, stable augmentation, stationary markers, stereo view, system description, system features, ultrasonic imaging, ultrasound images, ultrasound transducer, video cameras, video frame rate, water bath, workstations},
	pages = {30--39}
}

@inproceedings{palmer_mobile_2015,
	title = {Mobile {3D} augmented-reality system for ultrasound applications},
	doi = {10.1109/ULTSYM.2015.0488},
	abstract = {Ultrasound imaging is a highly valuable diagnostic tool. It is increasingly portable, provides real-time imaging of complex structures, and is considered safe. Yet, because ultrasound is a highly operator dependent modality the uptake of ultrasound within a broader range of medical contexts has been limited and hasn't made major inroads within the offices of General Practitioners, Midwives, and other non-specialists. Learning to effectively use ultrasound can easily take up to 12 months with direct expert supervision. To facilitate wider adoption of ultrasound technology we are attempting to determine if using augmented-reality can speed up the process of learning to use ultrasound by providing a patient specific correspondence between the ultrasound data acquired in real-time and a sufficiently detailed augmented 3D scene. We have established a tablet-based system for visualizing the heart within a patients body using augmented-reality techniques in conjunction with the streaming data provided by a GE Vivid E9 ultrasound machine. This system gives the operator visual feedback as to the location of the heart within the body, the anatomical features the echo plane is intersecting and if the operator is currently tracking the left-ventricle.},
	booktitle = {2015 {IEEE} {International} {Ultrasonics} {Symposium} ({IUS})},
	author = {Palmer, Cameron Lowell and Haugen, Bjørn Olav and Tegnander, Eva and Eik-Nes, Sturla H. and Torp, Hans and Kiss, Gabriel},
	month = oct,
	year = {2015},
	keywords = {Augmented reality, Biomedical imaging, Cameras, GE Vivid E9 ultrasound machine, Heart, Solid modeling, Three-dimensional displays, Ultrasonic imaging, anatomical feature intersection, augmented 3D scene, augmented reality, biomedical ultrasonics, cardiology, complex structure, data streaming, data visualisation, echo plane, heart visualisation, learning, learning (artificial intelligence), left ventricle tracking, media streaming, medical image processing, mobile 3D augmented reality system, natural scenes, operator visual feedback, tablet-based system, ultrasonic imaging, ultrasound applications, ultrasound imaging},
	pages = {1--4}
}

@inproceedings{perla_inspectar:_2016,
	title = {{InspectAR}: {An} {Augmented} {Reality} {Inspection} {Framework} for {Industry}},
	shorttitle = {{InspectAR}},
	doi = {10.1109/ISMAR-Adjunct.2016.0119},
	abstract = {With the advancement in camera technologies and data streaming protocols, AR based applications are proving to be an important aid for inspection, training and supervision tasks in various operations including automotive industry, education etc. We demonstrate an AR based re-configurable inspection framework that can be utilized in cross-domain applications such as maintenance and repair assistance in industrial inspection and automotive/avionics domain inspection, amongst others. A deep learning component detects parts viewed in inspector's Field-of-View (FoV) accurately and the corresponding inspection check-list can be prioritized based on detection results. The back-end of the framework is easily configurable for different applications where instructions can be directly imported and visually integrated with inspection type. Accurate recording of status of inspection is provided through evidence capturing of images, notes and videos. Our current framework supports all the Android based devices and will be demonstrated on Google Glass, Google Cardboard with smartphone, and Tablet with the help of 3D printer inspection use-case.},
	booktitle = {2016 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} ({ISMAR}-{Adjunct})},
	author = {Perla, Ramakrishna and Gupta, Gaurav and Hebbalaguppe, Ramya and Hassan, Ehtesham},
	month = sep,
	year = {2016},
	keywords = {3D printer inspection use-case, AR based applications, AR based reconfigurable inspection, Android (operating system), Android based devices, Augmented, Augmented reality, Glass, Google, Google Cardboard, Google Glass, H.5.1 [Information Interfaces and Presentation]: Artificial, InspectAR, Inspection, Object detection, Printers, Servers, Tablet, [Human-centered computing]: Ubiquitous and mobile computing—Ambient Intelligence I.4.8 [Computing Methodologies]: Image Processing and Computer Vi, and Virtual Realities—, augmented reality, augmented reality inspection, automatic optical inspection, camera technologies, cross-domain applications, data streaming protocols, deep learning component, evidence image capturing, image capture, inspection check-list, inspection type, inspector field-of-view, learning (artificial intelligence), part detection, printers, production engineering computing, smart phones, smartphone, three-dimensional printing},
	pages = {355--356}
}

@inproceedings{schwerdtfeger_using_2008,
	address = {New York, NY, USA},
	series = {{VRST} '08},
	title = {Using {Laser} {Projectors} for {Augmented} {Reality}},
	isbn = {978-1-59593-951-7},
	url = {http://doi.acm.org/10.1145/1450579.1450608},
	doi = {10.1145/1450579.1450608},
	abstract = {The paper explores the use of laser projectors as an alternative to head-mounted displays for Augmented Reality. We describe the development of an Augmented Reality Laser Projector and report on experiences setting up AR systems that use laser projectors, reasoning about several design criteria.},
	urldate = {2017-12-12},
	booktitle = {Proceedings of the 2008 {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology}},
	publisher = {ACM},
	author = {Schwerdtfeger, Björn and Pustka, Daniel and Hofhauser, Andreas and Klinker, Gudrun},
	year = {2008},
	keywords = {augmented reality, industrial augmented reality, laser projector},
	pages = {134--137}
}

@inproceedings{maynes-aminzade_youre_2003,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '03},
	title = {You'{Re} in {Control}: {A} {Urinary} {User} {Interface}},
	isbn = {978-1-58113-637-1},
	shorttitle = {You'{Re} in {Control}},
	url = {http://doi.acm.org/10.1145/765891.766108},
	doi = {10.1145/765891.766108},
	abstract = {The You're In Control system uses computation to enhance the act of urination. Sensors in the back of a urinal detect the position of impact of a stream of urine, enabling the user to play interactive games on a screen mounted above the urinal.},
	urldate = {2017-12-12},
	booktitle = {{CHI} '03 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Maynes-Aminzade, Dan and Raffle, Hayes},
	year = {2003},
	keywords = {augmented reality, entertainment, urinal},
	pages = {986--987}
}

@inproceedings{zhou_applying_2011,
	address = {New York, NY, USA},
	series = {{VRCAI} '11},
	title = {Applying {Spatial} {Augmented} {Reality} to {Facilitate} {In}-situ {Support} for {Automotive} {Spot} {Welding} {Inspection}},
	isbn = {978-1-4503-1060-4},
	url = {http://doi.acm.org/10.1145/2087756.2087784},
	doi = {10.1145/2087756.2087784},
	abstract = {In automotive manufacturing, the quality of spot welding on car bodies needs to be inspected frequently. Operators often only check different subsets of spots on different car bodies with a predetermined sequence. Currently, spot welding inspections rely on a printed drawing of the testing body, with the inspection points marked on this drawing. Operators have to locate the matching spot on the drawing and the body manually to perform the inspection. The manual inspection process suffers from inefficiencies and potential mistakes. This paper describes a system that projects visual data onto arbitrary surfaces for providing just-in-time information to a user in-situ within a physical work-cell. Spatial Augmented Reality (SAR) is the key technology utilized in our system. SAR facilitates presentation of projected digital Augmented Reality (AR) information on surfaces of car bodies. Four types of digital AR information are projected onto the surfaces of car body parts in structured work environments: 1) Location of spot welds; 2) Inspection methods; 3) Operation Description Sheet (ODS) information; 4) Visualization of weld locating methods. Various visualization methods are used to indicate the position of spot welds and the method used for spot welding inspection. Dynamical visualizations are used to assist operators to locate spot welds more easily. The SAR approach does not require additional special models in finding spot welds, but only needs knowledge of location of spot welds on the part. Our system allows operators becoming more effective and efficient to in performing proper inspections, by providing them the required information at the required time without the need to refer to paper-based manuals or computer terminals.},
	urldate = {2017-12-12},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Virtual} {Reality} {Continuum} and {Its} {Applications} in {Industry}},
	publisher = {ACM},
	author = {Zhou, Jianlong and Lee, Ivan and Thomas, Bruce and Menassa, Roland and Farrant, Anthony and Sansome, Andrew},
	year = {2011},
	keywords = {spatial augmented reality, spot welding inspection, visualization method},
	pages = {195--200}
}

@inproceedings{kanithi_immersive_2016,
	address = {New York, NY, USA},
	series = {{ICVGIP} '16},
	title = {Immersive {Augmented} {Reality} {System} for {Assisting} {Needle} {Positioning} {During} {Ultrasound} {Guided} {Intervention}},
	isbn = {978-1-4503-4753-2},
	url = {http://doi.acm.org/10.1145/3009977.3010023},
	doi = {10.1145/3009977.3010023},
	abstract = {Ultrasound (US) guided intervention is a surgical procedure where the clinician makes use of imaging in realtime, to track the position of the needle, and correct its trajectory for accurately steering it to the lesion of interest. However, the needle is visible in the US image, only when aligned in-plane with the scanning plane of the US probe. In practice, clinicians often use a mechanical needle guide, thus restricting their available degrees of freedom in the US probe movement. Alternatively, during free-hand procedure, they use multiple needle punctures to achieve this in-plane positioning. Our present work details an augmented reality (AR) system for patient comfort centric aid to needle intervention through an overlaid visualization of the needle trajectory on the US frame prior to its insertion. This is implemented by continuous visual tracking of the US probe and the needle in 3D world coordinate system using fiducial markers. The tracked marker positions are used to draw the needle trajectory and tip visualized in realtime to augment on the US feed. Subsequently, the continuously tracked US probe and needle, and the navigation assistance information, would be overlaid with the visual feed from a head mounted display (HMD) for generating totally immersive AR experience for the clinician.},
	urldate = {2017-12-12},
	booktitle = {Proceedings of the {Tenth} {Indian} {Conference} on {Computer} {Vision}, {Graphics} and {Image} {Processing}},
	publisher = {ACM},
	author = {Kanithi, Praveen Kumar and Chatterjee, Jyotirmoy and Sheet, Debdoot},
	year = {2016},
	keywords = {augmented reality, fiducial markers, head mounted display, image guided interventions, ultrasonography},
	pages = {65:1--65:8}
}

@inproceedings{jayaweera_enhanced_2017,
	address = {New York, NY, USA},
	series = {{IoTDI} '17},
	title = {Enhanced {Real}-{Time} {Machine} {Inspection} with {Mobile} {Augmented} {Reality} for {Maintenance} and {Repair}: {Demo} {Abstract}},
	isbn = {978-1-4503-4966-6},
	shorttitle = {Enhanced {Real}-{Time} {Machine} {Inspection} with {Mobile} {Augmented} {Reality} for {Maintenance} and {Repair}},
	url = {http://doi.acm.org/10.1145/3054977.3057302},
	doi = {10.1145/3054977.3057302},
	abstract = {We describe a system that we have built called ARIOT, an IoT (Internet of Things) enabled AR (Augmented Reality) wearable system to enhance the capabilities of machine operators and repairmen. ARIOT is a solution to the limited availability of highly capable and highly experienced machine inspectors in industrial environments. The outcome of this research and development project is an evaluation of the possibility of using a mobile Augmented Reality (AR) system, which communicates with various components of a machine to visualise real-time data pertaining to those components. For example, if there is an error or a potential error, the machine operator will be notified with a prompt to further instructions allowing a relatively inexperienced operator to learn on the job and for an expert to save valuable time. Also we focus on the modularity, extendability and security of communications within the system.},
	urldate = {2017-12-12},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Internet}-of-{Things} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Jayaweera, Malith and Wijesooriya, Indika and Wijewardana, Damith and De Silva, Tharindu and Gamage, Chandana},
	year = {2017},
	keywords = {Augmented Reality, Internet of Things, Real-Time Machine Inspection},
	pages = {287--288}
}

@book{deutsch_zfp_2010,
	address = {Wuppertal},
	title = {{ZfP} kompakt und verständlich - {Die} {Ultraschallprüfung}},
	volume = {1},
	isbn = {978-3-934255-52-4},
	publisher = {Castell-Verlag GmbH},
	author = {Deutsch, Volker and Platte, Michael and Deutsch, Wolfram A. Karl and Schuster, Volker and Vogt, Manfred},
	year = {2010}
}

@misc{htc_vive_2017,
	title = {{VIVE}™ {Deutschland} {\textbar} {VIVE} {Hardware} kaufen},
	url = {https://www.vive.com/de/product/},
	abstract = {Buy VIVE . Get the headset, two wireless controllers and two base stations enabling 360° room-scale motion-tracking when you order Vive},
	urldate = {2017-11-20},
	author = {HTC},
	month = nov,
	year = {2017}
}

@book{avgoustinov_towards_2017,
	title = {Towards multiscale simulation of non-destructive tests in virtual and augmented reality},
	abstract = {Most Non-Destructive Tests (NDT) rely on effects, which are invisible without special equipment - e.g. ultrasound, X-rays, electromagnetic fields. This fact complicates the understanding of the individual inspection techniques and their handling - e.g. during calibration, optimising and documenting of tests. Therefore, suitable methods and tools have to be developed in order to improve the understanding of NDT effects - e.g., by means of modelling of the processes with consequent simulation and visualisation of the results or the process model in virtual reality (VR). Uncomplicated accessibility and usability in both technical and economical sense are key aspects for these methods and tools in order to become generally accepted and well-established. One of the discussed problems is the simulation and visualisation of processes and even whole process chains, whereas the geometrical model data and the measurement data have to be overlayed for their better interpretation. A part of this problem - the overlaying of measurment data acquired by means of different techniques - is known as data fusion .},
	author = {Avgoustinov, N and Wolter, Bernd and Dobmann, Gerd and Boller, Christian},
	month = dec,
	year = {2017}
}

@book{dorner_virtual_2013,
	address = {Berlin Heidelberg},
	series = {{eXamen}.press},
	title = {Virtual und augmented reality ({VR}/{AR}): {Grundlagen} und {Methoden} der virtuellen und augmentierten {Realität}},
	isbn = {978-3-642-28902-6 3-642-28902-9},
	language = {ger},
	publisher = {Springer Vieweg},
	editor = {Dörner, Ralf and Broll, Wolfgang and Grimm, Paul and Jung, Bernhard},
	year = {2013},
	keywords = {(BIC Subject Heading)UYZG, (Produktform)Paperback / softback, (VLB-WN)1635: Hardcover, Softcover / Informatik, EDV/Anwendungs-Software, (Zielgruppe)Fachpublikum/ Wissenschaft, (Zielgruppe)Upper undergraduate, Aufsatzsammlung, Augmented Reality, Computer Graphics and Computer Vision, Erweiterte Realität, Human Computer Interaction, Interactive Systems, Virtual Reality, Virtuelle Realität}
}

@inproceedings{meyer_visualisation_2015,
	address = {Erding},
	series = {6},
	title = {Visualisation of {Ultrasonic} testing {Data} using {Augmented} {Reality}},
	copyright = {CC-BY},
	language = {english},
	publisher = {DGZIP},
	author = {Meyer, Jean and Rehbein, Jörg and De Freese, Jens and Holtmanspötter, Jens},
	year = {2015}
}

@misc{daqri_balancing_2017,
	title = {The {Balancing} {Act}: {Creating} a {Helpful} {UI} for {Augmented} {Reality}},
	shorttitle = {The {Balancing} {Act}},
	url = {https://medium.com/@DAQRI/the-balancing-act-creating-a-helpful-ui-for-augmented-reality-43be08bbe92e},
	abstract = {At DAQRI, we feel that one of the main advantages of augmented reality is its ability to keep a user present in their environment while…},
	urldate = {2017-11-20},
	journal = {Medium},
	author = {DAQRI},
	month = mar,
	year = {2017}
}

@book{craig_understanding_2013,
	address = {Oxford, UNITED STATES},
	title = {Understanding {Augmented} {Reality} : {Concepts} and {Applications}},
	isbn = {978-0-240-82410-9},
	url = {http://ebookcentral.proquest.com/lib/bibl/detail.action?docID=1183494},
	publisher = {Elsevier Science},
	author = {Craig, Alan B.},
	year = {2013}
}

@misc{bonasio_start-up_2017,
	title = {The {Start}-{Up} {Using} {Augmented} {Reality} to {Clean} {Up} {The} {Real} {World}},
	url = {http://techtrends.tech/tech-trends/cleaning-up-with-augmented-reality/},
	abstract = {AR-Check is a company providing enhanced commercial cleaning services through innovative use of Augmented/Mixed Reality Technology},
	urldate = {2017-11-20},
	journal = {Tech Trends},
	author = {Bonasio, Alice},
	month = may,
	year = {2017}
}
@inproceedings{aschenbrenner_industrial_2016,
	address = {New York, NY, USA},
	series = {{VRST} '16},
	title = {Industrial {Maintenance} with {Augmented} {Reality}: {Two} {Case} {Studies}},
	isbn = {978-1-4503-4491-3},
	shorttitle = {Industrial {Maintenance} with {Augmented} {Reality}},
	url = {http://doi.acm.org/10.1145/2993369.2996305},
	doi = {10.1145/2993369.2996305},
	abstract = {Remote maintenance of industrial manipulators often is performed via telephone support. Recent approaches in the context of the 'Industry 4.0' consider internet technologies and Augmented Reality (AR) to enhance situation awareness between external experts and local service technicians. We present two AR-based case studies: First, a mobile AR architecture based on optical see through glasses is used for an on-site local repair task. Second, a remote architecture based on a portable tablet PC and a high precision tracking system is used to realize an off-site expert access. The to-be-serviced machine is visualized inside of a large area similar to a machinery hall and can be inspected by the experts walking around this virtual plant using the tablet and perspectively correct rendering to understand the production process and the operation context. Both methods have been evaluated in first user studies.},
	urldate = {2017-11-20},
	booktitle = {Proceedings of the {22Nd} {ACM} {Conference} on {Virtual} {Reality} {Software} and {Technology}},
	publisher = {ACM},
	author = {Aschenbrenner, Doris and Latoschik, Marc Erich and Schilling, Klaus},
	year = {2016},
	keywords = {augmented reality, industrial internet, industry 4.0, maintenance, situation awareness},
	pages = {341--342}
}

@misc{bastian_htc_2016,
	title = {{HTC} {Vive}: {Valve} nennt durchschnittliche {Größe} der {Trackingfläche}},
	shorttitle = {{HTC} {Vive}},
	url = {https://vrodo.de/htc-vive-valve-nennt-durchschnittliche-groesse-der-trackingflaeche/},
	abstract = {Die Statistik von Valve, basierend auf den ersten zwei Monaten Virtual-Reality-Nutzung, zeigt: Room-Scale-VR wird gut angenommen.},
	urldate = {2017-11-20},
	author = {Bastian, Matthias},
	month = jun,
	year = {2016}
}

@misc{staff_oculus_2017,
	title = {Oculus {Rift} vs. {HTC} {Vive}: {Prices} are lower, but our favorite remains the same},
	shorttitle = {Oculus {Rift} vs. {HTC} {Vive}},
	url = {https://www.digitaltrends.com/virtual-reality/oculus-rift-vs-htc-vive/},
	abstract = {What happens when you put the Oculus Rift vs. HTC Vive in a war of specifications? There's only one way to find out.},
	urldate = {2017-11-20},
	author = {Staff, Digital Trends},
	month = sep,
	year = {2017}
}

@article{dorner_virtual_2016,
	title = {Virtual {Reality} und {Augmented} {Reality} ({VR}/{AR}) - {Auf} dem {Weg} von der {Niesche} zum {Massenmarkt}},
	volume = {39},
	issn = {0170-6012, 1432-122X},
	url = {https://link.springer.com/article/10.1007/s00287-014-0838-9},
	doi = {10.1007/s00287-014-0838-9},
	abstract = {No Abstract available for this article.},
	language = {de},
	number = {1},
	urldate = {2017-11-17},
	journal = {Informatik-Spektrum},
	author = {Dörner, Ralf and Broll, Wolfgang and Grimm, Paul and Jung, Bernhard},
	month = feb,
	year = {2016},
	pages = {30--37}
}

@book{tonnis_augmented_2010,
	address = {Berlin, Heidelberg},
	series = {Informatik im {Fokus}},
	title = {Augmented {Reality}},
	volume = {0},
	isbn = {978-3-642-14178-2 978-3-642-14179-9},
	url = {http://link.springer.com/10.1007/978-3-642-14179-9},
	urldate = {2017-11-17},
	publisher = {Springer Berlin Heidelberg},
	author = {Tönnis, Marcus},
	year = {2010},
	doi = {10.1007/978-3-642-14179-9}
}

@misc{okreylos_optical_2016,
	title = {Optical {Properties} of {Current} {VR} {HMDs}},
	url = {http://doc-ok.org/?p=1414},
	abstract = {With the first commercial version of the Oculus Rift (Rift CV1) now trickling out of warehouses, and Rift DK2, HTC Vive DK1, and Vive Pre already being in developers’ hands, it’s time f…},
	urldate = {2017-11-08},
	author = {{okreylos}},
	month = apr,
	year = {2016}
}

@inproceedings{haritos_mobile_2005,
	title = {A mobile application of augmented reality for aerospace maintenance training},
	volume = {1},
	doi = {10.1109/DASC.2005.1563376},
	abstract = {Aircraft maintenance technicians (AMTs) must obtain new levels of job task skill and knowledge to effectively work with modem computer-based avionics and advanced composite materials. Traditional methods of training, such as on-the-job training (OJT), may not have potential to fulfill the training requirements to meet future trends in aviation maintenance. A new instruction delivery system could assist AMTs with job task training and job tasks. The purpose of this research is to analyze the use of an augmented reality (AR) system as a training medium for novice AMTs. An AR system has the potential to enable job task training and job task guidance for the novice technician in a real world environment. An AR system could reduce the cost for training and retraining of AMTs by complementing human information processing and assisting with performance of job tasks. An AR system could eliminate the need to leave the aircraft for the retrieval of information from maintenance manuals for inspection and repair procedures. AR has the potential to supply rapid and accurate feedback to an AMT with any information that he/she needs to successfully complete a job task. New technologies that promote smaller computer-based systems make the application of a mobile AR system possible in the near future.},
	booktitle = {24th {Digital} {Avionics} {Systems} {Conference}},
	author = {Haritos, T. and Macchiarella, N. D.},
	month = oct,
	year = {2005},
	keywords = {Aerospace electronics, Aerospace materials, Aircraft manufacture, Application software, Augmented reality, Composite materials, Costs, Humans, Modems, On the job training, advanced composite materials, aerospace computing, aerospace maintenance training, aerospace simulation, aircraft maintenance, aircraft maintenance technicians, augmented reality, aviation maintenance, computer based training, computer-based avionics, human information processing, information retrieval, inspection procedure, instruction delivery system, job task guidance, job task skill, job task training, maintenance manuals, mobile application, on-the-job training, real world environment, repair procedure},
	pages = {5.B.3--5.1--9 Vol. 1}
}

@article{niehorster_accuracy_2017,
	title = {The {Accuracy} and {Precision} of {Position} and {Orientation} {Tracking} in the {HTC} {Vive} {Virtual} {Reality} {System} for {Scientific} {Research}},
	volume = {8},
	issn = {2041-6695},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5439658/},
	doi = {10.1177/2041669517708205},
	abstract = {The advent of inexpensive consumer virtual reality equipment enables many more researchers to study perception with naturally moving observers. One such system, the HTC Vive, offers a large field-of-view, high-resolution head mounted display together with a room-scale tracking system for less than a thousand U.S. dollars. If the position and orientation tracking of this system is of sufficient accuracy and precision, it could be suitable for much research that is currently done with far more expensive systems. Here we present a quantitative test of the HTC Vive’s position and orientation tracking as well as its end-to-end system latency. We report that while the precision of the Vive’s tracking measurements is high and its system latency (22 ms) is low, its position and orientation measurements are provided in a coordinate system that is tilted with respect to the physical ground plane. Because large changes in offset were found whenever tracking was briefly lost, it cannot be corrected for with a one-time calibration procedure. We conclude that the varying offset between the virtual and the physical tracking space makes the HTC Vive at present unsuitable for scientific experiments that require accurate visual stimulation of self-motion through a virtual world. It may however be suited for other experiments that do not have this requirement.},
	number = {3},
	urldate = {2017-10-28},
	journal = {i-Perception},
	author = {Niehorster, Diederick C. and Li, Li and Lappe, Markus},
	month = may,
	year = {2017},
	pmid = {28567271},
	pmcid = {PMC5439658}
}

@inproceedings{liu_investigation_2006,
	address = {Beijing, China},
	title = {Investigation on {Human}-{Simulation} {Intelligent} {Control} of the {Touch} {Force} in {Remote} {Welding} {Teleteaching}},
	url = {http://ieeexplore.ieee.org/document/1691747/},
	abstract = {In remote welding teleteaching process (RWTP), some factors, such as the high-frequency interference, the vibration of the robot arm, the change of the actual gravity center of welding tracing probe produced by the change of robot space posture, and the roughness of the welding groove, etc. This may strongly interfere with the force signal, reduce the reliability of touch force signal transmission, distort the force telepresence of welding operator, and consequently influence remote welding teleteaching quality. The paper, by analyzing force error target track on the foundation of the PID control model, brings forward human-simulation intelligent control (HSIC) strategy based on the force guiding. Its characteristic algorithm is established, including the characteristic model, characteristic identification, characteristic memory, multi-model control and multi-target decision-making. The experimental results show that the force error between fact phase track and idea phase track is lessened, the oscillating scope of touch force obviously reduced, the stability time of RWTP shortened, the efficiency and playback precision of the RWTP improved by comparing HSIC with PID control},
	booktitle = {First {International} {Conference} on {Innovative} {Computing}, {Information} and {Control} - {Volume} {I} ({ICICIC}'06)},
	author = {Liu, L.J. and Gao, H.M. and Zhang, G.G. and Wu, L.},
	month = oct,
	year = {2006}
}